{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import plotly.plotly as py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "MAX_LINES = 100\n",
    "if not os.path.exists(os.path.expanduser(\"~/.plotly/.credentials\")):\n",
    "    plotly_user = input(\"plotly username: \")\n",
    "    plotly_api_key = input(\"plotly api key: \")\n",
    "    plotly.tools.set_credentials_file(username=plotly_user, api_key=plotly_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_input_file = input(\"csv log input file: \")\n",
    "file = open(log_input_file, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_types = set()\n",
    "\n",
    "\n",
    "def clean(tables):\n",
    "    tables = tables.strip('`').strip('\\'').strip()\n",
    "    list_tables = tables.split(',')\n",
    "    for i in range(len(list_tables)):\n",
    "        list_tables[i] = remove_suffixes(list_tables[i], [' t0', ' t1', ' t2'])\n",
    "        list_tables[i] = remove_prefixes(list_tables[i], ['mysql.'])\n",
    "    list_tables = list(set(list_tables))\n",
    "    return list_tables\n",
    "\n",
    "\n",
    "def remove_suffixes(text, suffixes):\n",
    "    for suffix in suffixes:\n",
    "        if text.endswith(suffix):\n",
    "            text = text[:-len(suffix)].strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_prefixes(text, prefixes):\n",
    "    for prefix in prefixes:\n",
    "        if text.startswith(prefix):\n",
    "            text = text[len(prefix):].strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_tables_from_select(argument):\n",
    "    argument = argument.lower()\n",
    "    if 'from' in argument.lower():\n",
    "        return argument.split(' from ')[1].split(' group by ')[0].split(' order by ')[0].split(' where ')[0]\n",
    "    elif 'table_name = ' in argument:\n",
    "        return argument.split('table_name = ')[1]\n",
    "    elif 'from' in argument:\n",
    "        return argument.split('from')[1].split('where')[0]\n",
    "    else:\n",
    "        if argument not in ['select 1', 'select @@session.tx_isolation',\n",
    "                                     'select database()', 'select user()',\n",
    "                                     'select @@global.sync_binlog']:\n",
    "            if DEBUG:\n",
    "                print(\"No tables found:\", argument)\n",
    "        return None\n",
    "\n",
    "    \n",
    "def load_from(file, max_lines):\n",
    "    i = 0\n",
    "    data = []\n",
    "    \n",
    "    for i, line in enumerate(file):\n",
    "        parsed = line.split('\\\"')\n",
    "        parsed = [parsed[i] for i in range(len(parsed)) if i % 2 != 0]\n",
    "\n",
    "        if i == 0:\n",
    "            fields = parsed\n",
    "            print(fields)\n",
    "        elif max_lines is not None and i > max_lines:\n",
    "            break\n",
    "        else:\n",
    "            entry = {field: val for (field, val) in zip(fields, parsed)}\n",
    "            entry['operation_type'] = entry['argument'].split(' ')[0].upper()\n",
    "            operation_types.add(entry['operation_type'])\n",
    "            entry[entry['operation_type']] = 1\n",
    "            entry['tables'] = None\n",
    "\n",
    "            if entry['operation_type'] == 'SELECT':\n",
    "                entry['tables'] = extract_tables_from_select(entry['argument'])\n",
    "            elif entry['operation_type'] == 'UPDATE':\n",
    "                if 'SET' in entry['argument']:\n",
    "                    entry['tables'] = entry['argument'].split('SET')[0].split('UPDATE')[1]\n",
    "                else:\n",
    "                    print(entry['argument'])\n",
    "                    entry['tables'] = None\n",
    "            elif entry['operation_type'] == 'INSERT':\n",
    "                entry['tables'] = entry['argument'].split('INSERT INTO')[1].split('(')[0]\n",
    "            elif entry['operation_type'] == 'SHOW' and 'TABLES FROM ' in entry['argument']:\n",
    "                entry['tables'] = entry['argument'].split('TABLES FROM ')[1].split(' ')[0]\n",
    "            else:\n",
    "                if entry['argument'] != 'commit':\n",
    "                    continue\n",
    "\n",
    "            if entry['tables'] is not None:\n",
    "                entry['table_list'] = clean(entry['tables'])\n",
    "                for table in entry['table_list']:\n",
    "                    entry[table] = 1\n",
    "                entry['tables'] = (', ').join(entry['table_list'])\n",
    "\n",
    "            data.append(entry)\n",
    "\n",
    "    file.close()\n",
    "    return data\n",
    "\n",
    "data = load_from(file, max_lines = MAX_LINES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event_time'] = pd.to_datetime(df['event_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['operation_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tables'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = df.copy()\n",
    "timeseries.index = timeseries['event_time']\n",
    "del timeseries['event_time']\n",
    "timeseries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries_data(cols, colors=None):\n",
    "    if colors == None:\n",
    "        colors = [\"#{:06x}\".format(random.randint(0, 0xFFFFFF)) for i in range(len(cols))]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for col, color in zip(cols, colors):\n",
    "        series = timeseries[[col]].groupby(level=0).count().resample('5T').sum()\n",
    "        data.append(go.Scatter(x=series.index,\n",
    "                             y=series[col],\n",
    "                             name=col,\n",
    "                             line = dict(color = color),\n",
    "                             opacity = 0.8))\n",
    "    return data\n",
    "\n",
    "\n",
    "data = generate_timeseries_data([field for field in timeseries.columns if field in operation_types])\n",
    "\n",
    "layout = dict(\n",
    "    title='Time Series by Operation Type',\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=30,\n",
    "                     label='30m',\n",
    "                     step='minute',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=1,\n",
    "                     label='1h',\n",
    "                     step='hour',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=4,\n",
    "                     label='4h',\n",
    "                     step='hour',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=6,\n",
    "                     label='6h',\n",
    "                     step='hour',\n",
    "                     stepmode='backward'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(),\n",
    "        type='date'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbls = [field for field in timeseries.columns if field in ['dce_annotation_user_info', 'dce_aws_s3_archive_mapping', 'dce_metadata_synchronize',\n",
    "       'dce_metadata_synchronize_wf', 'dce_otherseries','dce_transcript_job_control', 'general_log limit 10',\n",
    "       'information_schema.innodb_sys_tables', 'information_schema.tables',\n",
    "       'matterhorn', 'mh_annotation', 'mh_capture_agent_state',\n",
    "       'mh_episode_asset', 'mh_episode_episode', 'mh_episode_version_claim',\n",
    "       'mh_group', 'mh_group_member', 'mh_host_registration', 'mh_incident',\n",
    "       'mh_job', 'mh_job_argument', 'mh_organization', 'mh_organization_node',\n",
    "       'mh_organization_property', 'mh_search', 'mh_service_registration',\n",
    "       'mh_user', 'mh_user_action', 'mh_user_ref', 'mh_user_session',\n",
    "       'rds_configuration', 'rds_heartbeat2', 'rds_history',\n",
    "       'rds_replication_status', 'sequence']]\n",
    "\n",
    "py.iplot(dict(data=generate_timeseries_data(tbls), layout=layout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGULAR Matplotlib chart\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "operations = ['SELECT', 'UPDATE', 'INSERT', 'SHOW']\n",
    "for operation in operations:\n",
    "    plt.plot(timeseries[operation].groupby(level=0).count(), label=operation)\n",
    "plt.ylabel(\"occurances\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Export data to json\n",
    "# df_json = df.copy().drop(columns=['table_list'])\n",
    "# data = df_json.to_json(orient='records')\n",
    "\n",
    "# with open('output.json', 'w') as outfile:\n",
    "#     json.dump(data, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
