{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import arrow\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk as bulk_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "MAX_LINES = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_input_file = input(\"csv log input file: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_types = set()\n",
    "\n",
    "def clean(tables):\n",
    "    tables = tables.strip('`').strip('\\'').strip()\n",
    "    list_tables = tables.split(',')\n",
    "    for i in range(len(list_tables)):\n",
    "        list_tables[i] = remove_suffixes(list_tables[i], [' t0', ' t1', ' t2'])\n",
    "        list_tables[i] = remove_prefixes(list_tables[i], ['mysql.'])\n",
    "    list_tables = list(set(list_tables))\n",
    "    return list_tables\n",
    "\n",
    "\n",
    "def remove_suffixes(text, suffixes):\n",
    "    for suffix in suffixes:\n",
    "        if text.endswith(suffix):\n",
    "            text = text[:-len(suffix)].strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_prefixes(text, prefixes):\n",
    "    for prefix in prefixes:\n",
    "        if text.startswith(prefix):\n",
    "            text = text[len(prefix):].strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_tables_from_select(argument):\n",
    "    argument = argument.lower()\n",
    "    if 'from' in argument.lower():\n",
    "        return argument.split(' from ')[1].split(' group by ')[0].split(' order by ')[0].split(' where ')[0]\n",
    "    elif 'table_name = ' in argument:\n",
    "        return argument.split('table_name = ')[1]\n",
    "    elif 'from' in argument:\n",
    "        return argument.split('from')[1].split('where')[0]\n",
    "    else:\n",
    "        if argument not in ['select 1', 'select @@session.tx_isolation',\n",
    "                                     'select database()', 'select user()',\n",
    "                                     'select @@global.sync_binlog']:\n",
    "            if DEBUG:\n",
    "                print(\"No tables found:\", argument)\n",
    "        return None\n",
    "\n",
    "    \n",
    "def load_from(file, max_lines):\n",
    "    reader = csv.DictReader(file)\n",
    "    \n",
    "    for i, entry in enumerate(reader, 1):\n",
    "        if max_lines is not None and i == max_lines:\n",
    "            break\n",
    "        else:\n",
    "            entry['operation_type'] = entry['argument'].split(' ')[0].upper()\n",
    "            operation_types.add(entry['operation_type'])\n",
    "            entry['tables'] = None\n",
    "\n",
    "            if entry['operation_type'] == 'SELECT':\n",
    "                entry['tables'] = extract_tables_from_select(entry['argument'])\n",
    "                entry['select'] = entry['argument'].split('WHERE')[0]\n",
    "            elif entry['operation_type'] == 'UPDATE':\n",
    "                if 'SET' in entry['argument']:\n",
    "                    entry['tables'] = entry['argument'].split('SET')[0].split('UPDATE')[1]\n",
    "                else:\n",
    "                    print(entry['argument'])\n",
    "                    entry['tables'] = None\n",
    "            elif entry['operation_type'] == 'INSERT':\n",
    "                entry['tables'] = entry['argument'].split('INSERT INTO')[1].split('(')[0]\n",
    "            elif entry['operation_type'] == 'SHOW' and 'TABLES FROM ' in entry['argument']:\n",
    "                entry['tables'] = entry['argument'].split('TABLES FROM ')[1].split(' ')[0]\n",
    "            else:\n",
    "                if entry['argument'] != 'commit':\n",
    "                    continue\n",
    "\n",
    "            if entry['tables'] is not None:\n",
    "                entry['tables'] = clean(entry['tables'])\n",
    "                \n",
    "            entry['event_time'] = arrow.get(entry['event_time']).datetime\n",
    "\n",
    "            action = { '_id': i, '_index': 'general_log', '_type': 'event' }\n",
    "            action.update(entry)\n",
    "            yield action\n",
    "                \n",
    "\n",
    "with gzip.open(log_input_file, \"rt\") as f:\n",
    "    es = Elasticsearch()\n",
    "    actions = load_from(f, max_lines = MAX_LINES)\n",
    "    bulk_index(es, actions, chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
